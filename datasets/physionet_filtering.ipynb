{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Physionet 2012 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import ma\n",
    "import seaborn as sns\n",
    "from itertools import chain\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils import np_utils\n",
    "from sklearn.utils import shuffle\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Physionet 2012 Challenge files\n",
    "source: https://www.physionet.org/content/challenge-2012/1.0.0/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!wget https://physionet.org/files/challenge-2012/1.0.0/set-a.zip\n",
    "!wget https://physionet.org/files/challenge-2012/1.0.0/set-b.zip\n",
    "\n",
    "!wget https://physionet.org/files/challenge-2012/1.0.0/Outcomes-a.txt\n",
    "!wget https://physionet.org/files/challenge-2012/1.0.0/Outcomes-b.txt\n",
    "\n",
    "!wget https://physionet.org/files/challenge-2012/1.0.0/set-c.tar.gz   \n",
    "!wget https://physionet.org/files/challenge-2012/1.0.0/Outcomes-c.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -u set-a.zip\n",
    "!unzip -u set-b.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = ['set-a', 'set-b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataframe(dataset):\n",
    "    txt_all = list()\n",
    "    for f in os.listdir(dataset):\n",
    "        with open(os.path.join(dataset, f), 'r') as fp:\n",
    "            txt = fp.readlines()\n",
    "\n",
    "        # get recordid to add as a column\n",
    "        recordid = txt[1].rstrip('\\n').split(',')[-1]\n",
    "        txt = [t.rstrip('\\n').split(',') + [int(recordid)] for t in txt]\n",
    "        txt_all.extend(txt[1:])\n",
    "    df = pd.DataFrame(txt_all, columns=['time', 'parameter', 'value', 'recordid'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_a = get_dataframe('set-a')\n",
    "df_b = get_dataframe('set-b')\n",
    "df = pd.concat([df_a, df_b])\n",
    "df.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_all = list()\n",
    "tar = tarfile.open(\"set-c.tar.gz\", \"r:gz\")\n",
    "for member in tar.getmembers():\n",
    "    f = tar.extractfile(member)\n",
    "    if f is not None:\n",
    "        txt = f.readlines()\n",
    "    \n",
    "        txt = [x.decode(\"utf-8\") for x in txt]\n",
    "        # get recordid to add as a column\n",
    "        recordid = txt[1].rstrip('\\n').split(',')[-1]\n",
    "        txt = [t.rstrip('\\n').split(',') + [int(recordid)] for t in txt]\n",
    "        txt_all.extend(txt[1:])\n",
    "df_c = pd.DataFrame(txt_all, columns=['time', 'parameter', 'value', 'recordid'])\n",
    "df_c = df_c[df_c[\"parameter\"] != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df, df_c])\n",
    "df.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df[\"recordid\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_outcomes(dataset_outcomes):\n",
    "    outcomes = pd.read_csv(dataset_outcomes)\n",
    "    outcomes.set_index('RecordID', inplace=True)\n",
    "    outcomes.index.name = 'recordid'\n",
    "    outcomes = outcomes.reset_index()\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes_a = get_df_outcomes('Outcomes-a.txt')\n",
    "df_outcomes_b = get_df_outcomes('Outcomes-b.txt')\n",
    "df_outcomes_c = get_df_outcomes('Outcomes-c.txt')\n",
    "df_outcomes = pd.concat([df_outcomes_a, df_outcomes_b, df_outcomes_c])\n",
    "df_outcomes.reset_index(inplace=True, drop=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_params = [\"RecordID\", \"Age\", \"Gender\", \"Height\", \"ICUType\", \"Weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"value\"] = df[\"value\"].apply(lambda x: float(x))\n",
    "df[\"hour\"] = df[\"time\"].apply(lambda x: int(x.split(':')[0]))\n",
    "df[\"minutes\"] = df[\"time\"].apply(lambda x: int(x.split(':')[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = df.loc[df['time'] == '00:00', :].copy()\n",
    "\n",
    "# retain only one of the 6 static vars:\n",
    "static_params = ['RecordID', 'Age', 'Gender', 'Height', 'ICUType', 'Weight']\n",
    "static_df = static_df.loc[df['parameter'].isin(static_params)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df = df.loc[~df.index.isin(static_df.index), :]\n",
    "\n",
    "static_df = static_df.groupby(['recordid', 'parameter'])[['value']].last()\n",
    "static_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df[\"recordid\"] = static_df[\"recordid\"].apply(lambda x: str(x))\n",
    "static_df = static_df.pivot(index='recordid', columns='parameter', values='value')\n",
    "static_df = static_df.reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### encode categorical variables: ICUType and Gender to one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ICUType one-hot encoding\n",
    "ICU_enc = OneHotEncoder()\n",
    "ICU_enc.fit(static_df[[\"ICUType\"]])\n",
    "ICU_type_one_hot = ICU_enc.transform(static_df[[\"ICUType\"]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df[\"ICUType=1.0\"]=ICU_type_one_hot[:,0]\n",
    "static_df[\"ICUType=2.0\"]=ICU_type_one_hot[:,1]\n",
    "static_df[\"ICUType=3.0\"]=ICU_type_one_hot[:,2]\n",
    "static_df[\"ICUType=4.0\"]=ICU_type_one_hot[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gender one-hot encoding\n",
    "# if gender value is -1.0, it is missing and we assign zero for all categories (female and male)\n",
    "gender_enc = {0.0: np.eye(2)[0], 1.0: np.eye(2)[1], -1.0: np.array([0., 0.])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_enc_array = static_df[\"Gender\"].apply(lambda x: gender_enc[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_enc_array = np.stack(gender_enc_array.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df[\"Gender=0.0\"]=gender_enc_array[:,0]\n",
    "static_df[\"Gender=1.0\"]=gender_enc_array[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df[\"recordid\"] = time_series_df[\"recordid\"].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove record ids that do not contain time series information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recordids_to_remove = static_df[~static_df[\"recordid\"].isin(time_series_df[\"recordid\"].tolist())][\"recordid\"].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df = static_df[~static_df[\"recordid\"].isin(recordids_to_remove)]\n",
    "time_series_df = time_series_df[~time_series_df[\"recordid\"].isin(recordids_to_remove)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "static_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_time_to_decimal_time(t):\n",
    "    t_strings = t.split(':')\n",
    "    decimal_time = float(t_strings[0]) + float(t_strings[1])/60\n",
    "    return round(decimal_time, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df[\"decimal_times\"] = time_series_df[\"time\"].apply(lambda x: convert_time_to_decimal_time(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hourly_time_groups(hour, mins):\n",
    "    if mins in range(1, 60):\n",
    "        new_time = str(hour + 1) + \":00\"\n",
    "    elif (hour == 0) and (mins == 0):\n",
    "        new_time = str(hour + 1) + \":00\"\n",
    "    elif (hour != 0) and (mins == 0):\n",
    "        new_time = str(hour) + \":00\"\n",
    "    return new_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_df[\"time_group\"] = time_series_df.apply(lambda x: get_hourly_time_groups(x[\"hour\"], x[\"minutes\"]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series_params =  [\"Albumin\", \"ALP\", \"ALT\", \"AST\", \"Bilirubin\", \"BUN\", \"Cholesterol\", \"Creatinine\", \\\n",
    "                      \"DiasABP\", \"FiO2\", \"GCS\", \"Glucose\", \"HCO3\", \"HCT\", \"HR\", \"K\", \"Lactate\", \"Mg\", \"MAP\", \\\n",
    "                      \"MechVent\", \"Na\", \"NIDiasABP\", \"NIMAP\", \"NISysABP\", \"PaCO2\", \"PaO2\", \"pH\", \"Platelets\", \\\n",
    "                      \"RespRate\", \"SaO2\", \"SysABP\", \"Temp\", \"TroponinI\", \"TroponinT\", \"Urine\", \"WBC\", \"Weight\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## aggregating the time steps to one hour bins\n",
    "## if two or more measurements were taken within the same hour bin, we take the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dict = defaultdict()\n",
    "median_dict = time_series_df.groupby([\"recordid\", \"time_group\", \"parameter\"])[\"value\"].median().to_dict()\n",
    "for k, v in median_dict.items():\n",
    "    timeseries_dict[k[0]] = defaultdict()\n",
    "\n",
    "for k, v in median_dict.items():\n",
    "    timeseries_dict[k[0]][k[1]] = defaultdict()\n",
    "\n",
    "for k, v in median_dict.items():\n",
    "    timeseries_dict[k[0]][k[1]][k[2]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = static_df.copy()\n",
    "final_df.pop(\"RecordID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_dict_GRU = defaultdict()\n",
    "median_dict_GRU = time_series_df.groupby([\"recordid\", \"decimal_times\", \"parameter\"])[\"value\"].median().to_dict()\n",
    "\n",
    "for k, v in median_dict_GRU.items():\n",
    "    timeseries_dict_GRU[k[0]] = defaultdict()\n",
    "\n",
    "for k, v in median_dict_GRU.items():\n",
    "    timeseries_dict_GRU[k[0]][k[1]] = defaultdict()\n",
    "\n",
    "for k, v in median_dict_GRU.items():\n",
    "    timeseries_dict_GRU[k[0]][k[1]][k[2]] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_per_time_group_GRU(record_dict, time):\n",
    "    time_tracked = record_dict[time]\n",
    "    param_list = list(time_tracked.keys())\n",
    "    day_one_hot = [[time_tracked[t], 0.0] if t in param_list else [0.0, 1.0] for t in time_series_params]\n",
    "    final = list(chain.from_iterable(day_one_hot))\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tracking_input_GRU(record_id):\n",
    "    record_dict = timeseries_dict_GRU[record_id]\n",
    "    tracking_times = list(record_dict.keys())\n",
    "    \n",
    "    final_list = [transform_input_per_time_group_GRU(record_dict, d) for d in tracking_times]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_input_tracking_one_hot = map(transform_tracking_input_GRU, static_df[\"recordid\"].unique())\n",
    "final_df[\"GRU_input_one_hot\"] = list(GRU_input_tracking_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_GRU_times(record_id):\n",
    "    record_dict = timeseries_dict_GRU[record_id]\n",
    "    tracking_times = list(record_dict.keys())\n",
    "    return tracking_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRU_times = map(get_GRU_times, static_df[\"recordid\"].unique())\n",
    "final_df[\"GRU_times\"] = list(GRU_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_intervals_to_keep = ['%s:00' % h for h in ([0] + list(range(1,49)))]\n",
    "time_intervals_to_keep = time_intervals_to_keep[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_input_per_time_group(record_dict, time_group):\n",
    "    tracking_times = list(record_dict.keys())\n",
    "    if time_group not in tracking_times:\n",
    "        final = [0.0, 1.0] * 37\n",
    "        return final\n",
    "    elif time_group in tracking_times:\n",
    "        time_tracked = record_dict[time_group]\n",
    "        param_list = list(time_tracked.keys())\n",
    "        day_one_hot = [[time_tracked[t], 0.0] if t in param_list else [0.0, 1.0] for t in time_series_params]\n",
    "        final = list(chain.from_iterable(day_one_hot))\n",
    "        return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_tracking_input(record_id):\n",
    "    if record_id in timeseries_dict.keys():\n",
    "        record_dict = timeseries_dict[record_id]\n",
    "        final_list = [transform_input_per_time_group(record_dict, d) for d in time_intervals_to_keep]\n",
    "    else:\n",
    "        final = [0.0, 1.0] * 37\n",
    "        final_list = np.array(final * 48).reshape(48, 74)\n",
    "        final_list = [list(x) for x in final_list]\n",
    "    return final_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"input_one_hot\"] = static_df[\"recordid\"].apply(lambda x: transform_tracking_input(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_outcomes[\"recordid\"] = df_outcomes[\"recordid\"].apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = pd.merge(final_df, df_outcomes, how=\"left\", left_on=\"recordid\", right_on=\"recordid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"In-hospital_death\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"In-hospital_death\"] = final_df[\"In-hospital_death\"].apply(lambda x: int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_balance_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get missingness fractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_missing_features_per_timestep(input_one_hot):\n",
    "    n_missing_features = []\n",
    "    input_one_hot = np.array(input_one_hot)\n",
    "    missing_matrix = input_one_hot[:,1::2]\n",
    "    count_list = np.sum(missing_matrix, axis=1).tolist()\n",
    "    return count_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"n_missing_features\"] = final_df[\"input_one_hot\"].apply(lambda x: count_missing_features_per_timestep(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get percentage of missing values per input - output pair\n",
    "final_df[\"total_missingness\"] = final_df[\"n_missing_features\"].apply(lambda x: sum(x))\n",
    "n_features = 37\n",
    "n_time_steps = 48\n",
    "n_total = n_features * n_time_steps\n",
    "final_df[\"total_frac_missingness\"] = final_df[\"total_missingness\"] / n_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"total_frac_missingness\"].mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"total_frac_missingness\"].min() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"total_frac_missingness\"].max() * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare dataset for GRU-D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_true_features(input_one_hot):\n",
    "    input_one_hot = np.array(input_one_hot)\n",
    "    true_features = input_one_hot[:,::2]\n",
    "    return true_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask(input_one_hot):\n",
    "    input_one_hot = np.array(input_one_hot)\n",
    "    mask = 1 - input_one_hot[:,1::2]\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"X\"] = final_df[\"GRU_input_one_hot\"].apply(get_true_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"M\"] = final_df[\"GRU_input_one_hot\"].apply(get_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"GRU_lengths\"] = final_df[\"X\"].apply(lambda x: len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df[\"tracked_sum_per_feature\"] = final_df[\"M\"].apply(lambda x: x.sum(axis=0))\n",
    "final_df[\"feature_sum_per_patient\"] =  final_df[\"X\"].apply(lambda x: x.sum(axis=0)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_first_missing_with_mean(masks, values, mean_features_dict):\n",
    "    # get indexes of the features that were not tracked \n",
    "    # where any first value is missing\n",
    "    zero_idx = np.where(masks[0] == 0)[0]\n",
    "    for idx in zero_idx:\n",
    "        # replace first missing time step value of missing feature to empirical mean of that feature\n",
    "        values[0,:][idx] = mean_features_dict[idx]\n",
    "    return values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emperical_mean_features_dict(df):\n",
    "    # get the emperical mean of the nonzero values in the dataset\n",
    "    feature_sum_all_patients = np.stack(df[\"feature_sum_per_patient\"])\n",
    "    features_sum = feature_sum_all_patients.sum(axis=0)\n",
    "    empirical_mean_features_dict = defaultdict()\n",
    "    for i in range(0, 37):\n",
    "        emp_mean = features_sum[i] / np.count_nonzero(feature_sum_all_patients[:,i])\n",
    "        if np.isnan(emp_mean):\n",
    "            emp_mean = 0.0\n",
    "        empirical_mean_features_dict[i] = emp_mean\n",
    "    return empirical_mean_features_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocessing final data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_data_(x, input_mask, mean, std, encoder):\n",
    "    \"\"\"\n",
    "    Rescale the dataset based on the mean and std of the training set\n",
    "    Args:\n",
    "        x: A np.array of several np.array with shape (t_i, d).\n",
    "        mean: A np.array of shape (d,).\n",
    "        std: A np.array of shape (d,).\n",
    "    Returns:\n",
    "        Same shape as x with rescaled values.\n",
    "    \"\"\"\n",
    "    if encoder == \"GRU\":\n",
    "        input_values = x[:,::2]\n",
    "        input_mask = 1 - x[:,1::2]\n",
    "        # have np.nan instead of 0\n",
    "        mdata = ma.masked_array(input_values, mask=~input_mask.astype(bool))\n",
    "    elif encoder == \"GRUD\":\n",
    "        mdata = ma.masked_array(x, mask=~input_mask.astype(bool))\n",
    "    r = np.asarray([(xx - mean[np.newaxis, :]) / std[np.newaxis, :] for xx in mdata])\n",
    "    r = r.reshape(mdata.shape)\n",
    "    \n",
    "    if encoder == \"GRU\":\n",
    "        x[:,::2] = r\n",
    "        return x\n",
    "    elif encoder == \"GRUD\":\n",
    "        return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_mean_std(x):\n",
    "    input_values = x[:,:,::2]\n",
    "    input_mask = 1 - x[:,:,1::2]\n",
    "    mdata = ma.masked_array(input_values, mask=~input_mask.astype(bool))\n",
    "    train_std = mdata.std(axis=(0, 1))\n",
    "    mean_std = mdata.mean(axis=(0, 1))\n",
    "    return mean_std, train_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_static_params = [\"RecordID\", \"Age\", \"Gender=0.0\", \"Gender=1.0\", \\\n",
    "                     \"Height\", \"ICUType=1.0\", \"ICUType=2.0\", \"ICUType=3.0\", \"ICUType=4.0\", \"Weight\"]\n",
    "static_input_cols = new_static_params[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train, validation, and test set\n",
    "train, test = train_test_split(final_df, test_size=0.2, stratify=final_df[\"In-hospital_death\"], random_state=21)\n",
    "train, val = train_test_split(train, test_size=0.2, stratify=train[\"In-hospital_death\"], random_state=21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the emperical mean from the training dataset\n",
    "train_emperical_mean_dict = get_emperical_mean_features_dict(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for GRU-D: update first value with the empirical mean if the first value is missing\n",
    "train_x_updated_first_values = train.apply(lambda x: replace_first_missing_with_mean(x[\"M\"], x[\"X\"], train_emperical_mean_dict), axis=1)\n",
    "val_x_updated_first_values = val.apply(lambda x: replace_first_missing_with_mean(x[\"M\"], x[\"X\"], train_emperical_mean_dict), axis=1)\n",
    "test_x_updated_first_values = test.apply(lambda x: replace_first_missing_with_mean(x[\"M\"], x[\"X\"], train_emperical_mean_dict), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# missing values of height and weight are replaced by the mean of the training dataset\n",
    "train_aux_df = train[[\"Height\", \"Weight\"]]\n",
    "train_aux_df = train_aux_df.replace(-1, np.nan)\n",
    "train_mean = train_aux_df.mean()\n",
    "train[[\"Height\", \"Weight\"]] = train_aux_df.fillna(train_mean)\n",
    "\n",
    "val_aux_df = val[[\"Height\", \"Weight\"]]\n",
    "val_aux_df = val_aux_df.replace(-1, np.nan)\n",
    "val[[\"Height\", \"Weight\"]] = val_aux_df.fillna(train_mean)\n",
    "\n",
    "test_aux_df = test[[\"Height\", \"Weight\"]]\n",
    "test_aux_df = test_aux_df.replace(-1, np.nan)\n",
    "test[[\"Height\", \"Weight\"]] = test_aux_df.fillna(train_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### standardize final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = defaultdict()\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    data[split] = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare training set\n",
    "X_train = np.stack(train[\"input_one_hot\"])\n",
    "#mean: A np.array of shape (d,)\n",
    "#std: A np.array of shape (d,)\n",
    "mean_train, std_train = get_train_mean_std(X_train) \n",
    "\n",
    "# rescale non-missing values to non-zero mean\n",
    "# prepare GRU input data\n",
    "data[\"train\"][\"X_train\"] = np.array([rescale_data_(x, 0, mean_train, std_train, \"GRU\") for x in X_train])\n",
    "\n",
    "# prepare GRUD input data\n",
    "X_train_GRUD = np.array(train[\"X\"])\n",
    "data[\"train\"][\"M\"] = np.array(train[\"M\"])\n",
    "data[\"train\"][\"X\"] = np.array([rescale_data_(x, data[\"train\"][\"M\"][i], mean_train, std_train, \"GRUD\") for i, x in enumerate(X_train_GRUD)])\n",
    "\n",
    "data[\"train\"][\"GRU_lengths\"] = np.array(train[\"GRU_lengths\"])\n",
    "data[\"train\"][\"GRU_times\"] = np.array(train[\"GRU_times\"])\n",
    "\n",
    "# standardize static input\n",
    "X_train_aux = train[static_input_cols]\n",
    "scaler = StandardScaler().fit(X_train_aux)\n",
    "data[\"train\"][\"X_aux\"] = scaler.transform(X_train_aux)\n",
    "\n",
    "# turn output classes into categorical one-hot-encoding\n",
    "data[\"train\"][\"y\"] = np_utils.to_categorical(np.array(train[\"In-hospital_death\"]), num_classes=2).astype(float)\n",
    "data[\"train\"][\"y_classes\"] = np.array(train[\"In-hospital_death\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare validation set\n",
    "X_val = np.stack(val[\"input_one_hot\"])\n",
    "data[\"val\"][\"X_val\"] = np.array([rescale_data_(x, 0, mean_train, std_train, \"GRU\") for x in X_val])\n",
    "\n",
    "# prepare GRUD input data\n",
    "X_val_GRUD = np.array(val[\"X\"])\n",
    "data[\"val\"][\"M\"] = np.array(val[\"M\"])\n",
    "data[\"val\"][\"X\"] = np.array([rescale_data_(x, data[\"val\"][\"M\"][i], mean_train, std_train, \"GRUD\") for i, x in enumerate(X_val_GRUD)])\n",
    "\n",
    "data[\"val\"][\"GRU_lengths\"] = np.array(val[\"GRU_lengths\"])\n",
    "data[\"val\"][\"GRU_times\"] = np.array(val[\"GRU_times\"])\n",
    "\n",
    "# standardize static input\n",
    "X_val_aux = val[static_input_cols]\n",
    "data[\"val\"][\"X_aux\"] = scaler.transform(X_val_aux)\n",
    "\n",
    "# turn output classes into categorical one-hot-encoding\n",
    "data[\"val\"][\"y\"] = np_utils.to_categorical(np.array(val[\"In-hospital_death\"]), num_classes=2).astype(float)\n",
    "data[\"val\"][\"y_classes\"] = np.array(val[\"In-hospital_death\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare test set\n",
    "X_test = np.stack(test[\"input_one_hot\"])\n",
    "data[\"test\"][\"X_test\"] = np.array([rescale_data_(x, 0, mean_train, std_train, \"GRU\") for x in X_test])\n",
    "\n",
    "# prepare GRUD input data\n",
    "X_test_GRUD = np.array(test[\"X\"])\n",
    "data[\"test\"][\"M\"] = np.array(test[\"M\"])\n",
    "data[\"test\"][\"X\"] = np.array([rescale_data_(x, data[\"test\"][\"M\"][i], mean_train, std_train, \"GRUD\") for i, x in enumerate(X_test_GRUD)])\n",
    "\n",
    "data[\"test\"][\"GRU_lengths\"] = np.array(test[\"GRU_lengths\"])\n",
    "data[\"test\"][\"GRU_times\"] = np.array(test[\"GRU_times\"])\n",
    "\n",
    "# standardize static input\n",
    "X_test_aux = test[static_input_cols]\n",
    "data[\"test\"][\"X_aux\"] = scaler.transform(X_test_aux)\n",
    "\n",
    "# turn output classes into categorical one-hot-encoding\n",
    "data[\"test\"][\"y\"] = np_utils.to_categorical(np.array(test[\"In-hospital_death\"]), num_classes=2).astype(float)\n",
    "data[\"test\"][\"y_classes\"] = np.array(test[\"In-hospital_death\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('physionet2012.pickle', 'wb') as handle:\n",
    "    pickle.dump(data, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
